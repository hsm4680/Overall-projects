#Mounting your google Drive
from google.colab import drive
drive.mount('/content/gdrive')


import os

#구글 드라이브와 colab 연동
os.chdir('/content/gdrive/My Drive/Colab Notebooks/')
current_path = os.getcwd()
main_dir = os.path.join(current_path, '2021-data-creator-camp-1030-1')
os.makedirs(main_dir, exist_ok=True)



# Importing important packages
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import os
import random
%matplotlib inline

## Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score

import warnings 
warnings.simplefilter('ignore')

# 유니코드 깨짐현상 해결
mpl.rcParams['axes.unicode_minus'] = False

## Random Seed 고정
def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
random_state = 42
seed_everything(random_state)

# 팀 이름 설정
team_name = '진리관227호'

train =  pd.read_csv(main_dir+'/train.csv')
test = pd.read_csv(main_dir+'/test.csv')
submission = pd.read_csv(main_dir+'/submission.csv')

#데이터 형태 확인
print('## train feature dataset info()')
print(train.info())


# 결측값 확인하기
print('훈련데이터 결측수 = {} \n테스트 데이터 결측수 = {}'.format(
              train.isnull().all().sum(),
              test.isnull().all().sum()))

## 데이터 내용 확인하기
train.head(3)

## 데이터 통계값 확인하기
train.describe()

##EDA
## 상관분석
plt.figure(figsize=(24,24))
sns.set(font_scale=2)
sns.heatmap(train.corr(),
               annot= True,
               annot_kws={"size": 20})

 ## 일원 분석
for col in train.columns:
    plt.figure(figsize=(10,5))
    if col in ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend', 'Revenue']:
        mes = train[col].value_counts()
        plt.figure(figsize=(10,5))
        sns.countplot(train[col], order = mes.index)        
        xval = -.41
        for index, value in mes.items():
            plt.text(x = xval, y = value + 50, s = str(value))
            xval += 1.03
    else:
        sns.distplot(x = train[col])
    plt.title(col)
    plt.show()

## 이원 분석
for col in train.columns.drop('Revenue'):
    plt.figure(figsize=(10,5))
    if train[col].dtype in ['object', 'bool']:
        sns.countplot(x = col, data = train, hue = 'Revenue')
    else:
        sns.boxplot(x = 'Revenue',y = col, data = train)
    plt.title(col)
    plt.show()


## ex1) 이상치 제거 함수
def get_outlier(df=None, column=None, alpha = 25, weight=1.5):
  # target 값과 상관관계가 높은 열을 우선적으로 진행
  quantile_lo = np.percentile(df[column].values, alpha)
  quantile_up = np.percentile(df[column].values, 100-alpha)

  IQR = quantile_up - quantile_lo
  IQR_weight = IQR*weight
  
  lowest = quantile_lo - IQR_weight
  highest = quantile_up + IQR_weight
  
  outlier_idx = df[column][ (df[column] < lowest) | (df[column] > highest) ].index
  return outlier_idx

  ## 이상치 제거, 훈련 데이터만 적용한다.
outlier_col = []
for col in outlier_col:
    remove_idx = get_outlier(train, col)
    if len(remove_idx) > 0:
        train.drop(remove_idx, axis=0, inplace=True)
train.index = range(len(train))    

## ex2) 더미화, 훈련 데이터와 테스트 데이터 모두 적용해야 한다.
dummy_col = ['Month','OperatingSystems','Browser','Region','TrafficType','VisitorType']
dummy_df = pd.get_dummies(train[dummy_col].astype(str))
drop_col = ['Month_Feb','VisitorType_Other']
dummy_df = dummy_df.drop(drop_col, axis = 1)
train = train.drop(dummy_col, axis = 1)
train=pd.concat([train,dummy_df],axis=1)

dummy_col = ['Month','OperatingSystems','Browser','Region','TrafficType','VisitorType']
dummy_df = pd.get_dummies(test[dummy_col].astype(str))
drop_col = ['Month_Feb','VisitorType_Other']
dummy_df = dummy_df.drop(drop_col, axis = 1)
test = test.drop(dummy_col, axis = 1)
test=pd.concat([test,dummy_df],axis=1)
test.head()

##데이터 스케일링

from sklearn.preprocessing import RobustScaler

# 변형 객체 생성
robust_scaler = RobustScaler()

# 훈련데이터의 모수 분포 저장
robust_scaler.fit(train)

# 훈련 데이터 스케일링
train_scaled = robust_scaler.transform(train)

##랜덤 포레스트
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
clf2=RandomForestClassifier(n_estimators=20)
clf2=clf.fit(train.drop(['Revenue'], axis = 1),train['Revenue'])
dot2_clf=tree.export_graphviz(clf2,out_file=None)
graph=graphviz.Source(dot2_clf)
graph

##모델 학습시키기
X = train.drop(['Revenue'], axis = 1)
y = train['Revenue']
le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state = 42, stratify= y)

threshold = 0.5
model = RandomForestClassifier(n_estimators=20)
model.fit(X_train, y_train)
print(model.get_params())
y_prob = model.predict_proba(X_valid)
y_pred = [0 if p > threshold else 1 for p in y_prob[:,0]]

threshold = 0.5
model = LogisticRegression()
model.fit(X_train, y_train)
print(model.get_params())
y_prob = model.predict_proba(X_valid)
y_pred = [0 if p > threshold else 1 for p in y_prob[:,0]]

##평가 지표 확인

## 검증 데이터 정답 확인
cmat = confusion_matrix(y_valid, y_pred)
plt.figure(figsize=(10,8))
plt.title("Confusion Matrix")
sns.heatmap(cmat, annot = True, fmt='',cmap='Blues')
plt.show()

## 검증 데이터 스코어 확인
acc = accuracy_score(y_valid, y_pred)
auc = roc_auc_score(y_valid, y_pred)
f1 = f1_score(y_valid, y_pred)
print(f"검증 데이터 정확도: {acc*100:.3f}%")
print(f"검증 데이터 AUC: {auc:.3f}")
print(f"검증 데이터 F1값: {f1:.3f}")

##구매로 이어지는 피쳐 중요도 확인
## 피쳐 중요도 확인
try:
  feature_importance = np.abs(model.coef_[0])
except:
  feature_importance = model.feature_importances_


data={'feature_names':X_train.columns,'feature_importance':feature_importance}
fi_df = pd.DataFrame(data)
fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

plt.figure(figsize=(10,8))
sns.barplot(x=fi_df['feature_importance'][:10], y=fi_df['feature_names'][:10])
plt.title('TOP 10 FEATURE IMPORTANCE')

plt.xlabel('FEATURE IMPORTANCE')
plt.ylabel('FEATURE NAMES')    
plt.show()

